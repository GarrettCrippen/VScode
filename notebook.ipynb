{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Research question and Motivation**\r\n",
    "\r\n",
    "<img src=\"Screenshots/c9.png\" width='480'>\r\n",
    "\r\n",
    "# Motivation\r\n",
    "*   I really want to use this api which comes from HaveIbeenPwned because I personally had my private information stolen and part of large data breaches. Typically you can put your phone number or email and find if you have been the victim of a data breach or even multiple. One of the main proponents of data breaches are passwords because they are incredibly valuable especially given people resuse passwords.\r\n",
    "\r\n",
    "*   HaveIbeenPwned allows you to search if a password has been breached and if so it provides the number of times it has been breached. So I found it interesting if I combined the idea of passwords and the names of the most famous athletes because I also like sports\r\n",
    "\r\n",
    "# Topic at hand\r\n",
    "* My hypothesis, is that athletes that are more famous are more likely to have some variation of their name used as a password. In other words, the more popular the athelte is, the more times that the corresponding passwords will show up under a data breach.(Assuming data breaches are mostly random) To represent fame, I aggregate different features ,such as total earning through endorsements, followers on social media and as well as the sport that they play.\r\n",
    "\r\n",
    "\r\n",
    "A sha1 encoding of a password is a 20 byte hash value, used for checking authenticity. HIBP(HaveIbeenPwned) api provides a search by range by the first 5 bytes of the hash as the key and it outputs the remaining 15 bytes appended with a : and the number of data breaches corresponding to many passwords with the same key. After finding the transforming each name eg.cristanoronalod to cristianoronalado25, I can compute the hash and then call a get(url prepended with the first 5 characters of the sha1). The later 15 characters on the list indicates a hit.\r\n",
    "![SegmentLocal](Screenshots/c3.gif \"segment\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Gathering**\r\n",
    "===============\r\n",
    "* I collected data from [Espn's world fame 100(2019)](https://www.espn.com/espn/feature/story/_/id/26113613/espn-world-fame-100-2019) which presents the 100 most famous athletes during 2019. I opted to use the selenium webdriver module because some of the html is java script loaded, so if I used get from the requests module, it would not show any of the information that I want. Afterwards, I did utlized beautiful soup to parse the html for thumbnails of the players.From the thumbnails, I was able to find, their name,(rating calculated by ESPN), total endorsements, follower count,sport and country."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"Screenshots/c1.png\" width='480'>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Since the Espn website only provides 100 athletes, I figured it would be beneificial to find another website or data set to scrape. This was pretty difficult and lacked forsight as I was pigeonholed into athlete endorsement and followers statistics from 2019. Eventualli I found [opendorse's top 100 highest paid athletes](https://opendorse.com/blog/top-100-highest-paid-athlete-endorsers-2019/) This website included some new features such as a description,estimated value per social media post,and notable endorsement deals,so I decided to only take the features that overlapped with the additional athletes that were not previously listed. Furthermore I opted to drop the country and rating/search score columns from my dataFrame since that feature is unique to the Espn website."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Cleaning**\r\n",
    "==============="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"Screenshots/c5.png\" width='480'>\r\n",
    "\r\n",
    "\r\n",
    "# Endorsements and Followers\r\n",
    "\r\n",
    "* In the Espn article, I needed to convert all columns to numeric. For the endorsements, I found that I could delete the \\$ sign by taking a slice that omits the first character. For converting the suffix such as m to millions or k for thousands, I used a regular expression to extract the suffix and put it as its own column in the dataframe. Afterwards, I removed the suffix from the endorsements column and then replaced the now numeric value in endorsements corresponding to the value of the suffix multiplied by its current value.Lastly, I dropped the suffix columns because they were not neccesary. The followers was very similar to the endorsements, other than the fact that It does not need the removal of the $ sign.\r\n",
    "-----------------\r\n",
    "`df['suffix1']=df['endorsements'].str.extract(r'([mk])+')`  \\Extract suffix\r\n",
    "\r\n",
    "`df['endorsements']=(df['endorsements'].str[1:].replace(r'[mk]+$','',regex=True))` \\remove suffix\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "-------------------------------------------------------------------------------------------------------\r\n",
    "\r\n",
    "`df['endorsements']=pandas.to_numeric(df['endorsements'],errors='coerce')`\r\n",
    "\r\n",
    "`df.loc[df['suffix1']=='m','endorsements']=df['endorsements']*10**6`                \\Calculate actual value\r\n",
    "\r\n",
    "`df.loc[df['suffix1']=='k','endorsements']=df['endorsements']*10**3`                 \\Calculate actual value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical data cleaning (name,sport,country)\r\n",
    "* For the general case, I found that categorical data was appended with a bunch of white space characters, especially tabs, so I used the lambda strip to strip all white space characters.\r\n",
    "`df[df.columns]=df.apply(lambda x: x.str.strip())` \\\\strip white space characters\r\n",
    "* For the sport, I used a replace to delete the unicode bullet that was prepended to every sport, but I also suppose I could have just taken a slice of the string.\r\n",
    "* One instance of American Football was mispelled with a lower case f\r\n",
    "* One name contained the latin u, which was omitted, so I had to reinsert it\r\n",
    "---------------------\r\n",
    "\r\n",
    "`df['sport']=df['sport'].str.replace('â€¢','')`       \\\\remove bullet point\r\n",
    "\r\n",
    "`df.loc[df['sport'=='American football'],'sport']='American Football'` \\\\fix typo\r\n",
    "\r\n",
    "`df.loc[df['name']=='thomas-mller','name']='thomas-muller'` \\\\insert u\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Cleaning(Cont...)**\r\n",
    "==============="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"Screenshots/c6.png\" width='480'>\r\n",
    "\r\n",
    "# Endorsements and Followers\r\n",
    "\r\n",
    "* For the endorsements, I split the string with : as the delimeter and took the 1st element, then I took a slice that excluded the dollar sign and removed all comma marks. For the twitter followers, I also split the string with the : delimeter and took the 1st element and then did the same technique I used on the ESPN website by extracting the suffix and multipliying it's equivalent value by the numeric value of the root. The one difference is that this website uses, the capital M and capital K for the suffixes, so I had to change my regular expression.\r\n",
    "----------\r\n",
    "`endorsements2=(en.text).split('$')[1].replace(',','').strip()` \\remove \\$ sign and commas\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical features(name, sport)\r\n",
    "\r\n",
    "* To get the name, I also use split, but with the | as the delimeter and the first occurance of a period as the delimeter. I also made the decision to remove all dashes,commas and periodsd in names, such as in Otto Porter, Jr. to simplify their names.\r\n",
    "\r\n",
    "* In the sport, they did not explcitly put the sport, such as in the Espn article, so I did found all the unique leagues, such as ATP and PGA tour and replaced them with their respective sport. I got the sport from getting the element after the | delimeter in the name.\r\n",
    "-------------------------\r\n",
    "`name2=player_thumbnail2.text.split('|')[0].split('.',1)[1].strip().replace(' ','-').replace('.','-').replace(',','-').lower()` \\name\r\n",
    "\r\n",
    "-------------------------------\r\n",
    "`df2['sport'].replace('La Liga','Soccer',inplace = True)`   \\These are only two of several\r\n",
    "\r\n",
    "`df2['sport'].replace('ATP Tour','Tennis',inplace = True)`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\r\n",
    "import matplotlib\r\n",
    "import pandas as pd\r\n",
    "from athletes_df import *\r\n",
    "\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "df=create_df()\r\n",
    "df['total_breaches']=0\r\n",
    "df['unique_breaches_by_name']=0\r\n",
    "df['name'].replace('-','',regex=True,inplace = True)\r\n",
    "\r\n",
    "f= open('breaches.txt','r')\r\n",
    "for line in f.readlines():\r\n",
    "    breached_data = line.split(':')\r\n",
    "    df.set_index('name')\r\n",
    "    df.loc[df['name']==(breached_data[0].strip()),'total_breaches']+=int(breached_data[2].strip())\r\n",
    "    df.loc[df['name']==(breached_data[0].strip()),'unique_breaches_by_name']+=1\r\n",
    "f.close()\r\n",
    "\r\n",
    "f= open('breaches2.txt','r')\r\n",
    "for line in f.readlines():\r\n",
    "    breached_data = line.split(':')\r\n",
    "    df.set_index('name')\r\n",
    "    df.loc[df['name']==(breached_data[0].strip()),'total_breaches']+=int(breached_data[2].strip())\r\n",
    "    df.loc[df['name']==(breached_data[0].strip()),'unique_breaches_by_name']+=1\r\n",
    "f.close()\r\n",
    "\r\n",
    "df.drop(columns={'rating','country'},inplace = True) #Data Cleaning are all NAN vallues for 101-157\r\n",
    "print(df.head(150))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'endorsements'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'endorsements'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15056/3406501657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_breaches'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'unique_breaches_by_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\playt\\Documents\\VScode\\athletes_df.py\u001b[0m in \u001b[0;36mcreate_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m    \u001b[1;31m#$ ensures that m is the end of entry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m    \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'suffix1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'endorsements'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'([mk])+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m    \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'endorsements'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'endorsements'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[mk]+$'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m    \u001b[1;31m#df['endorsements']=pandas.to_numeric(df['endorsements'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3453\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3455\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'endorsements'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Visualization**\r\n",
    "====================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## total_breaches by name\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['total_breaches'].describe())\r\n",
    "df.plot(kind='bar',x='name',y='total_breaches',fontsize='25',figsize=(80,20),title='Distribution of total_breaches',legend = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Let's drop ninja because he is a very common dictionary wordfirst name with no last name, making then both extreme outliers. Hopefully after this, or graph will depict some sort of trend and will have a significantly lower standard deviation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df=df.drop(df.index[df['name']=='ninja'])\r\n",
    "df=df.drop(df.index[df['name']=='oscar'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## total_breaches by name'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['total_breaches'].describe())\r\n",
    "df.plot(kind='bar',x='name',y='total_breaches',fontsize='25',figsize=(80,20),title='New Distribution of total_breaches',legend = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**As you can see, this graph looks like a better scale and it depicts a positive skewedness. If we look at the describe, the median is only 1/18 of the mean and the standard deviation is much larger than the mean which indicates high variability. It is also notable that tombrady has an exceptionally high data_breaches amongst its peers and it is the global maximum, despite being closer to the middle of the chart. Since my dataset exhibits high variablity I've opted to not removing tom brady although it is almost 7 standard deviations above the mean.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Follower Count vs. Total_breaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['followers'].describe())\r\n",
    "df2=df.sort_values(by='followers',ascending=False)\r\n",
    "df2.plot(kind='bar',x='followers',y='total_breaches',fontsize='25',figsize=(80,20),title='Follower Count vs Total_breaches',legend = True)\r\n",
    "#print(df2[df2['followers'].isna()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This model is not as clear.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Endorsements vs. Total_breaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['endorsements'].describe())\r\n",
    "df3=df.sort_values(by='endorsements',ascending=False)\r\n",
    "df3.plot(kind='bar',x='endorsements',y='total_breaches',fontsize='25',figsize=(80,20),title='Endorsements vs Total_breaches',legend = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df['sport'].describe())\r\n",
    "df4=df.groupby('sport').mean()\r\n",
    "df4.plot.bar(y='total_breaches')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**I do not think the sport is a usable feature because data is not large enough, so small sports with really popular athletes will appear as extreme outliers. For instance, there is only one instance of Moto GP and he happens to have quite a bit of data_breaches associated with his name**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Machine Learning**\r\n",
    "==============="
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.neighbors import KNeighborsRegressor\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.ensemble import VotingRegressor\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the dataset (90%-10%)\r\n",
    "\r\n",
    "# Let's start with one feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train, test = train_test_split(df, test_size=0.1)\r\n",
    "\r\n",
    "X_train=train[['endorsements']]\r\n",
    "Y_train=train['total_breaches']\r\n",
    "\r\n",
    "X_test=test[['endorsements']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = make_pipeline(\r\n",
    "          StandardScaler(),\r\n",
    "          LinearRegression()\r\n",
    ")\r\n",
    "\r\n",
    "pipeline.fit(X=X_train, y=Y_train)\r\n",
    "\r\n",
    "#10 folds\r\n",
    "scores = cross_val_score(pipeline,X=X_train,y=Y_train,cv=10)\r\n",
    "print(f'mean cross_val_score: {np.mean(scores)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Display MEAN RMSE**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results=test.copy(deep=True)\r\n",
    "results['predicted']=pipeline.predict(X_test)\r\n",
    "rms = mean_squared_error(results['total_breaches'], results['predicted'], squared=False)\r\n",
    "\r\n",
    "print(results[['endorsements','total_breaches','predicted']])\r\n",
    "print(f'MEAN RMSE: {rms}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K Nearest Neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = make_pipeline(\r\n",
    "          StandardScaler(),\r\n",
    "          KNeighborsRegressor(n_neighbors=int(np.sqrt(train.shape[0])))\r\n",
    ")\r\n",
    "\r\n",
    "pipeline.fit(X=X_train, y=Y_train)\r\n",
    "\r\n",
    "scores = cross_val_score(pipeline,X=X_train,y=Y_train,cv=10)\r\n",
    "print(f'mean cross_val_score: {np.mean(scores)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Both Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train, test = train_test_split(df, test_size=0.1)\r\n",
    "\r\n",
    "X_train=train[['endorsements','followers']]\r\n",
    "Y_train=train['total_breaches']\r\n",
    "\r\n",
    "X_test=test[['endorsements','followers']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* This is better with two features, but still not great"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline1 = make_pipeline(\r\n",
    "          StandardScaler(),\r\n",
    "          LinearRegression()\r\n",
    ")\r\n",
    "pipeline1.fit(X=X_train, y=Y_train)\r\n",
    "pipeline2 = make_pipeline(\r\n",
    "          StandardScaler(),\r\n",
    "          KNeighborsRegressor(n_neighbors=int(np.sqrt(train.shape[0])))\r\n",
    ")\r\n",
    "pipeline.fit2(X=X_train, y=Y_train)\r\n",
    "\r\n",
    "\r\n",
    "ensemble_model_tips = VotingRegressor([\r\n",
    "    (\"linear\", pipeline1.predict()), \r\n",
    "    (\"knn\", pipeline2.predict())\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.1 64-bit"
  },
  "interpreter": {
   "hash": "7ecd72385a9fa31fc82f83457b1757c613c10947a3805d55e820470c19790996"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}